# BKC-SAV-Solution

Syst√®me de Service Apr√®s Vente intelligent pour BKC (Blockchain + Burger King Company) avec assistante IA Isabelle et dashboard de gestion.

## Description

Ce projet combine une **interface de service client intelligente** et un **dashboard de gestion CRM** pour une entreprise e-commerce. L'assistante virtuelle **Isabelle** traite automatiquement les demandes clients en utilisant un mod√®le de langage local, tandis que le dashboard permet de g√©rer la base de donn√©es clients, commandes et articles.

## Fonctionnalit√©s

### ü§ñ Assistante IA Isabelle
- **Traitement automatique** des e-mails clients
- **R√©ponses contextuelles** bas√©es sur l'historique client
- **Int√©gration avec Ollama** (mod√®le : `adrienbrault/nous-hermes2pro:Q3_K_M`)
- **Interface web moderne** avec formulaire de contact
- **Gestion des commandes** et suivi client

### üìä Dashboard CRM
- **Visualisation des donn√©es** clients, commandes et articles
- **Gestion CRUD compl√®te** (Create, Read, Update, Delete)
- **Calculs automatiques** de montants de commandes
- **Analyses statistiques** (produits les plus vendus)
- **Export de donn√©es** au format CSV
- **Interface Streamlit** moderne et interactive

## Architecture

```
BKC-SAV-solution/
‚îú‚îÄ‚îÄ serveur.py          # API FastAPI pour l'assistante Isabelle
‚îú‚îÄ‚îÄ dashboard.py        # Dashboard Streamlit pour la gestion CRM
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ index.html     # Interface web du service client
‚îú‚îÄ‚îÄ requirements.txt    # D√©pendances Python
‚îî‚îÄ‚îÄ README.md          # Documentation
```

## Pr√©requis

### 1. Ollama et mod√®le LLM
Installez Ollama et t√©l√©chargez le mod√®le :
```bash
# Installation d'Ollama (voir https://ollama.ai)
ollama pull adrienbrault/nous-hermes2pro:Q3_K_M
```

### 2. Python 3.8+
Assurez-vous d'avoir Python 3.8 ou plus r√©cent install√©.

## Installation

1. **Clonez le repository :**
```bash
git clone <repository-url>
cd BKC-SAV-solution
```

2. **Installez les d√©pendances :**
```bash
pip install -r requirements.txt
```

3. **D√©marrez Ollama :**
```bash
ollama serve
```

## Utilisation

### üöÄ Lancement du service client (Isabelle)

```bash
# D√©marrer l'API FastAPI
uvicorn serveur:app --reload --host 0.0.0.0 --port 8000
```

Acc√©dez √† l'interface √† : `http://localhost:8000`

**Fonctionnalit√©s disponibles :**
- Envoi d'e-mails de demande de support
- R√©ponses automatiques personnalis√©es par Isabelle
- Recherche automatique dans la base clients
- Cr√©ation/mise √† jour de commandes

### üìä Lancement du dashboard CRM

```bash
# D√©marrer le dashboard Streamlit
streamlit run dashboard.py
```

Acc√©dez au dashboard √† : `http://localhost:8501`

**Fonctionnalit√©s disponibles :**
- **Visualisation** : Clients, Commandes, Articles, Contacts
- **Ajout** : Nouveaux clients, commandes, articles
- **Suppression** : Gestion des donn√©es obsol√®tes
- **Analytics** : Montants par commande, top produits
- **Export** : T√©l√©chargement CSV de toutes les donn√©es

## Base de donn√©es

Le syst√®me utilise **SQLite** avec les tables suivantes :

### Table `client`
```sql
id INTEGER PRIMARY KEY AUTOINCREMENT
email TEXT UNIQUE
nom TEXT
prenom TEXT
```

### Table `commande`
```sql
id INTEGER PRIMARY KEY AUTOINCREMENT
date TEXT
montant REAL
nb_articles INTEGER
id_client INTEGER (FOREIGN KEY)
```

### Table `article_commande`
```sql
id INTEGER PRIMARY KEY AUTOINCREMENT
id_commande INTEGER
nom_article TEXT
quantite INTEGER
prix_unitaire REAL
```

### Table `contact_client`
```sql
id INTEGER PRIMARY KEY AUTOINCREMENT
id_client INTEGER
date_contact TEXT
message TEXT
reponse TEXT
```

## Configuration

### Mod√®le LLM
Le mod√®le utilis√© est configur√© dans `serveur.py` :
```python
OLLAMA_URL = "http://localhost:11434/api/generate"
MODEL = "adrienbrault/nous-hermes2pro:Q3_K_M"
```

### Base de donn√©es
```python
DB_PATH = "crm_ecommerce.db"
```

## API Endpoints

### Service Client (FastAPI)
- `GET /` - Interface web du service client
- `POST /contact` - Traitement des demandes clients

### Dashboard (Streamlit)
Interface web compl√®te accessible via le navigateur.

## Exemples d'utilisation

### 1. Simulation d'une demande client
```
From: client@example.com
Subject: Probl√®me commande
Body: Bonjour, je souhaite passer une commande de 3 burgers et 2 frites. Merci !
```

**R√©ponse d'Isabelle :**
```
Bonjour Client,

Merci pour votre message. Votre commande #1 est bien prise en charge. 
Cependant, pour finaliser votre commande de burgers et frites, j'aurais 
besoin de quelques pr√©cisions : quel type de burgers souhaitez-vous, 
quelle taille pour les frites, et quel mode de livraison pr√©f√©rez-vous ?

Isabelle - Service client BKC.
```

### 2. Analyse des ventes
Le dashboard permet de :
- Voir les produits les plus vendus en temps r√©el
- Calculer automatiquement les montants de commandes
- Exporter les donn√©es pour des analyses externes

## S√©curit√©

- **CORS** configur√© pour les environnements de d√©veloppement
- **Validation** des entr√©es utilisateur
- **Gestion d'erreurs** robuste avec Ollama
- **Base de donn√©es** locale (SQLite)

## D√©veloppement

### Structure du code
- **`serveur.py`** : API FastAPI, logique m√©tier, int√©gration Ollama
- **`dashboard.py`** : Interface Streamlit, visualisations, CRUD
- **`templates/index.html`** : Frontend service client (HTML/CSS/JS)

### Ajout de fonctionnalit√©s
1. **Nouveaux endpoints** : Modifiez `serveur.py`
2. **Nouveaux graphiques** : Ajoutez dans `dashboard.py`
3. **Nouvelles tables** : Mettez √† jour `init_db()` dans `serveur.py`

## D√©pendances

- **FastAPI** (>=0.104.0) : Framework web API
- **Uvicorn** (>=0.24.0) : Serveur ASGI
- **Streamlit** (>=1.28.0) : Dashboard interactif
- **Requests** (>=2.31.0) : Communication avec Ollama
- **Pandas** (>=2.1.0) : Manipulation de donn√©es
- **Jinja2** (>=3.1.0) : Templates HTML
- **SQLite3** : Base de donn√©es (inclus avec Python)

## D√©ploiement

### D√©veloppement
```bash
# Terminal 1 : Service client
uvicorn serveur:app --reload

# Terminal 2 : Dashboard
streamlit run dashboard.py

# Terminal 3 : Ollama
ollama serve
```

### Production
```bash
# Service client (production)
uvicorn serveur:app --host 0.0.0.0 --port 8000

# Dashboard (production)
streamlit run dashboard.py --server.port 8501 --server.address 0.0.0.0
```

## Troubleshooting

### Probl√®mes courants

1. **Ollama non accessible :**
   - V√©rifiez que `ollama serve` est en cours d'ex√©cution
   - V√©rifiez l'URL dans `OLLAMA_URL`

2. **Mod√®le non trouv√© :**
   ```bash
   ollama pull adrienbrault/nous-hermes2pro:Q3_K_M
   ```

3. **Base de donn√©es corrompue :**
   ```bash
   rm crm_ecommerce.db  # Supprime et recr√©e au red√©marrage
   ```

4. **Port d√©j√† utilis√© :**
   ```bash
   # Changer les ports
   uvicorn serveur:app --port 8001
   streamlit run dashboard.py --server.port 8502
   ```

## Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

## Contribution

Les contributions sont les bienvenues ! N'h√©sitez pas √† :
- Signaler des bugs
- Proposer des am√©liorations
- Soumettre des pull requests
- Am√©liorer la documentation

## Auteurs

D√©velopp√© pour BKC (Blockchain + Burger King Company) avec l'assistante virtuelle Isabelle.

## Support

Pour toute question ou probl√®me :
1. V√©rifiez la section Troubleshooting
2. Consultez les logs des serveurs
3. V√©rifiez la configuration d'Ollama
4. Testez la connectivit√© des services
